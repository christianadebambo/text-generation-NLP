{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with Markov Chains\n",
    "\n",
    "In this experiment, we will be generating text by applying Markov Chains. This means we will be generating text based on the idea that the next possible word can be predicted using N previous words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from random import choice\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    \"\"\"\n",
    "    Read the contents of a file and perform text cleaning operations.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): The path to the file to be read.\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned text contents of the file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\", encoding='UTF-8') as file:\n",
    "        contents = file.read().replace('\\n\\n',' ').replace('[edit]', '').replace('\\ufeff', '').replace('\\n', ' ').replace('\\u3000', ' ')\n",
    "    return contents\n",
    "\n",
    "# Read File:\n",
    "# The `read_file` function takes a filename as input and reads the contents of the file.\n",
    "# It performs several text cleaning operations to remove unwanted characters and formatting.\n",
    "\n",
    "text = read_file('../root/input/book.txt')\n",
    "\n",
    "text_start = [m.start() for m in re.finditer('VOLUME ONE', text)]\n",
    "text_end = [m.start() for m in re.finditer('END OF THE PROJECT GUTENBERG EBOOK THE COUNT OF MONTE CRISTO', text)]\n",
    "text = text[text_start[1]:text_end[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-order Markov Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_dict(corpus):\n",
    "    \"\"\"\n",
    "    Collects words from the given corpus and creates a Markov chain dictionary.\n",
    "    \n",
    "    Args:\n",
    "        corpus (str): The input text corpus.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A Markov chain dictionary where each word is mapped to a list of words that can follow it.\n",
    "    \"\"\"\n",
    "    text_dict = {}\n",
    "    words = corpus.split(' ')\n",
    "    for i in range(len(words)-1):\n",
    "        if words[i] in text_dict:\n",
    "            text_dict[words[i]].append(words[i+1])\n",
    "        else:\n",
    "            text_dict[words[i]] = [words[i+1]]\n",
    "    \n",
    "    return text_dict\n",
    "\n",
    "def generate_text(words, limit=100):\n",
    "    \"\"\"\n",
    "    Generates text using the Markov chain dictionary of words.\n",
    "    \n",
    "    Args:\n",
    "        words (dict): A Markov chain dictionary of words.\n",
    "        limit (int): The maximum number of words in the generated text.\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    first_word = random.choice(list(words.keys()))\n",
    "    markov_text = first_word\n",
    "    while len(markov_text.split(' ')) < limit:\n",
    "        next_word = random.choice(words[first_word])\n",
    "        first_word = next_word\n",
    "        markov_text += ' ' + next_word\n",
    "    \n",
    "    return markov_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dignified; it impossible that proceed to look of three last struggle?” asked the fruit you on the court, are certain of all the steward disappeared. By the sugar question?” “Understand, sir, go!” exclaimed the most points of the count could not a cordial farewell, sir, that the banker; M. de la Révolution, and pain of a question. “No, no,” answered La Carconte when he rose and disappeared before you. I have enumerated and went to be, selfishness—it is quite recovered herself. “Allow me to you will dash out and brutally thrust forth, one-half per annum for my happiness or at the night I believe, my wife’s brother is at La Carconte. The count and both chased cups on some fair peasants had glanced through the multitude of documents relative to remain in a further description, and distract my pocket-book, the direction of the Rue Meslay, No. 34. They searched; they said the tears start when the square cracked without you know the new-comer left word of July,” replied coldly: “You met through the son of his _mocoletto_ from now to that the first floor. Albert eagerly, thereby incurring a man, Maximilian; thank me—for what?” “May I see Dantès over which\n"
     ]
    }
   ],
   "source": [
    "# Generate Markov chain dictionary from the text corpus\n",
    "word_pairs = collect_dict(text)\n",
    "\n",
    "# Generate text using the Markov chain model\n",
    "markov_text = generate_text(word_pairs, 200)\n",
    "\n",
    "# Print the generated text\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(words, limit=100):\n",
    "    \"\"\"\n",
    "    Generate text using a Markov chain based on the provided word dictionary.\n",
    "\n",
    "    Args:\n",
    "        words (dict): Dictionary of word pairs where each word is a key and the associated values are possible next words.\n",
    "        limit (int): Maximum number of words to generate.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text.\n",
    "\n",
    "    \"\"\"\n",
    "    capitalized_keys = [i for i in words.keys() if len(i) > 0 and i[0].isupper()]\n",
    "    first_word = random.choice(capitalized_keys)\n",
    "    markov_text = first_word\n",
    "    while len(markov_text.split(' ')) < limit:\n",
    "        next_word = random.choice(words[first_word])\n",
    "        first_word = next_word\n",
    "        markov_text += ' ' + next_word\n",
    "    \n",
    "    return markov_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mondego.” “You have any other with the paper of the Rue de Villefort no peculiar dress.” “Indeed,” said she, “how can tell me, I speak five minutes passed through a southern climes. When their former than on the count with its two hundred, two patrons with another portrait. It is gone! Gone, never forgotten. There could this costume—a ball was the purse still equal sufferer made so entirely in so lately at Albano, Tivoli, had chosen, and believe it was a seat into the Legion of his physical deformity. On the president. “‘Yes, sir.’—‘Who is seen twenty years, beneath the unhappy strangers, who, after rigorously paying his promise, but I am known, and, until the Porta San Giacomo.” “What!” exclaimed Franz withdrew to my mother earth.” It is clear morning to him weight; then, Count nor chest, and which were you come from?” asked Monte Cristo saved yourself with a strong-minded, upright young girl, “I mean to it seemed in behalf of the practiced all he remained listening to the never-varying smile upon the agitated by these caves might give you take—a glass opposite; it to withhold the frightful effect of despair. He would denounce you do you through her\n"
     ]
    }
   ],
   "source": [
    "markov_text = generate_text(word_pairs, 200)\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second-order Markov Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_dict(corpus):\n",
    "    \"\"\"\n",
    "    Collect word pairs from the corpus and create a dictionary of next words.\n",
    "    \"\"\"\n",
    "    text_dict = {}\n",
    "    words = corpus.split(' ')\n",
    "    for i in range(len(words)-2):\n",
    "        if (words[i], words[i+1]) in text_dict:\n",
    "            text_dict[(words[i], words[i+1])].append(words[i+2])\n",
    "        else:\n",
    "            text_dict[(words[i], words[i+1])] = [words[i+2]]\n",
    "    \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(words, limit = 100):\n",
    "    \"\"\"\n",
    "    Generate text using word pairs from the given dictionary.\n",
    "    \"\"\"\n",
    "    capitalized_keys = [i for i in words.keys() if len(i[0]) > 0 and i[0][0].isupper()]\n",
    "    first_key = random.choice(capitalized_keys)\n",
    "\n",
    "    markov_text = ' '.join(first_key)\n",
    "    while len(markov_text.split(' ')) < limit:\n",
    "        next_word = random.choice(words[first_key])\n",
    "        first_key = tuple(first_key[1:]) + tuple([next_word])\n",
    "        markov_text += ' ' + next_word\n",
    "    \n",
    "    return markov_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions and answers followed in the year of constant agony came back with a little distance. This would have been constantly seeking him, but all in capital order, because I was present during your long absence from Paris, and if you please; you may believe me to her throbbing heart, and before she saw no one?” “Phantoms are visible in the presence of children that have rendered me superior to any useless cause. Oh, great city, it is only about a hundred louis, and which was Teresa.” “The chief’s mistress?” “Yes. The Frenchman made some curious observations on this particular night the undertakers had executed their melancholy office, and asked the doctor added that his resolution to undertake, and had sent for a ship as the mysterious disappearance of Mercédès. “You were saying, sir——” said the unknown with a lusty stride soon traversed the space of his knife open, he rapidly ripped up the loss it occasioned me, but do you allude?” “Yes, we do; you see there.” And he stepped into their heads always filled with 30,000 pounds of gunpowder. “Near the merchant’s signature there was, indeed, strange in this corner. The time of the postscript, apparently with great\n"
     ]
    }
   ],
   "source": [
    "word_pairs = collect_dict(text)\n",
    "markov_text = generate_text(word_pairs, 200)\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing (to improve punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_dict(corpus):\n",
    "    \"\"\"\n",
    "    Collect word pairs from the given corpus and create a dictionary of next words.\n",
    "    \"\"\"\n",
    "    text_dict = {}\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "    for i in range(len(words)-2):\n",
    "        if (words[i], words[i+1]) in text_dict:\n",
    "            text_dict[(words[i], words[i+1])].append(words[i+2])\n",
    "        else:\n",
    "            text_dict[(words[i], words[i+1])] = [words[i+2]]\n",
    "    \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(words, limit = 100):\n",
    "    \"\"\"\n",
    "    Generate text using the given word pairs dictionary.\n",
    "    \"\"\"\n",
    "    capitalized_keys = [i for i in words.keys() if len(i[0]) > 0 and i[0][0].isupper()]\n",
    "    first_key = random.choice(capitalized_keys)\n",
    "    markov_text = ' '.join(first_key)\n",
    "    while len(markov_text.split(' ')) < limit:\n",
    "        next_word = random.choice(words[first_key])\n",
    "\n",
    "        first_key = tuple(first_key[1:]) + tuple([next_word])\n",
    "        markov_text += ' ' + next_word\n",
    "        \n",
    "    for i in ['.', '?', '!', ',']:\n",
    "        markov_text = markov_text.replace(' .', '.').replace(' ,', ',').replace(' !', '!').replace(' ?', '?').replace(' ;', ';')\n",
    "    return markov_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Maximilian, to confess that what you do not know the best grace possible; she was awaiting M. de Saint-Méran. Valentine had taken for an investigation, which he has given me, by order of Charles III., and that of living, and who rush with the habits and customs of the family. ” Monte Cristo that M. de Morcerf. ” “ That can only be surprised at his feet with sudden and factitious joy soon forsook the young people were shaking hands with him. ” “ Ah, madame, ” but his voice, “ your grandfather says he. “ But, by seven or eight horses, your excellency. ” “ You are a physician, and in the multitude of things the laugh would decidedly be against him, and the Count of Monte Cristo, “ we will take your brother Emmanuel. “ You removed this stone very carelessly; but now to Sunday morning. “ Give us a treat, Danglars? ” said he. “ Haydée, that the island was utterly absorbed in the East, madame, ” returned Dantès. There\n"
     ]
    }
   ],
   "source": [
    "word_pairs = collect_dict(text)\n",
    "markov_text = generate_text(word_pairs, 200)\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher-order Markov Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', '”', 'said', 'he', ',', '“'), 114),\n",
       " (('?', '”', '“', 'Yes.', '”', '“'), 99),\n",
       " ((',', '”', 'said', 'Monte', 'Cristo', ','), 95),\n",
       " ((',', '”', 'said', 'the', 'count', ','), 93),\n",
       " ((',', '”', 'he', 'said', ',', '“'), 58),\n",
       " (('”', 'said', 'the', 'count', ',', '“'), 57),\n",
       " (('”', 'said', 'Monte', 'Cristo', ',', '“'), 57),\n",
       " (('”', 'said', 'Monte', 'Cristo', '.', '“'), 55),\n",
       " (('”', '“', 'Well', ',', 'then', ','), 46),\n",
       " (('”', 'said', 'Monte', 'Cristo', ';', '“'), 43),\n",
       " (('.', '“', 'Well', ',', '”', 'said'), 42),\n",
       " ((',', '”', 'said', 'Albert', ',', '“'), 41),\n",
       " ((',', '”', 'said', 'Monte', 'Cristo', '.'), 41),\n",
       " (('?', '”', '“', 'No.', '”', '“'), 39),\n",
       " (('”', '“', 'Yes', ',', '”', 'said'), 37),\n",
       " ((',', '”', 'said', 'Monte', 'Cristo', ';'), 37),\n",
       " ((',', '”', 'said', 'the', 'young', 'man'), 35),\n",
       " ((',', '”', 'said', 'Madame', 'de', 'Villefort'), 35),\n",
       " (('?', '”', '“', 'Yes', ',', '”'), 33),\n",
       " (('”', 'said', 'the', 'young', 'man', ','), 30)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = nltk.word_tokenize(text)\n",
    "n_grams = ngrams(tokenized_text, 6)\n",
    "Counter(n_grams).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_dict(corpus):\n",
    "    text_dict = {}\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "\n",
    "    for i in range(len(words)-6):\n",
    "        key = tuple(words[i:i+6])\n",
    "        if key in text_dict:\n",
    "            text_dict[key].append(words[i+6])\n",
    "        else:\n",
    "            text_dict[key] = [words[i+6]]\n",
    "        \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who was it you were talking with over there? ” she asked. “ Oh, madame, ” said the count, “ have you ever seen me fire a pistol? ” “ Never. ” “ Well, we have time; look. ” Monte Cristo took the pistols he held in his hand when Mercédès entered, and fixing an ace of clubs against the iron plate, with four shots he successively shot off the four sides of the club. At each shot Morrel turned pale. He examined the bullets with which Monte Cristo performed this dexterous feat, and saw that they were no larger than buckshot. “ It is astonishing, ” said he. “ I am the Abbé Faria, born at Rome. I was for twenty years Cardinal Spada ’ s secretary; I was arrested, why, I know not, toward the beginning of the year 1811; since then I have demanded my liberty from the Italian and French government. ” “ Why from the French government? ” “ Because I was arrested at Piombino, and I presume that,\n"
     ]
    }
   ],
   "source": [
    "word_pairs = collect_dict(text)\n",
    "markov_text = generate_text(word_pairs, 200)\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_dict(corpus, n_grams):\n",
    "    \"\"\"\n",
    "    Collect n-grams from the corpus to create a dictionary.\n",
    "    \"\"\"\n",
    "    text_dict = {}\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "    for j in range(1, n_grams + 1):\n",
    "        sub_text_dict = {}\n",
    "        for i in range(len(words)-n_grams):\n",
    "            key = tuple(words[i:i+j])\n",
    "            if key in sub_text_dict:\n",
    "                sub_text_dict[key].append(words[i+n_grams])\n",
    "            else:\n",
    "                sub_text_dict[key] = [words[i+n_grams]]\n",
    "        text_dict[j] = sub_text_dict\n",
    "    \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_next_word(key_id, min_length):\n",
    "    \"\"\"\n",
    "    Get the next word given a key_id and minimum word length.\n",
    "    \"\"\"\n",
    "    for i in range(len(key_id)):\n",
    "        if key_id in word_pairs[len(key_id)]:\n",
    "            if len(word_pairs[len(key_id)][key_id]) >= min_length:\n",
    "                return random.choice(word_pairs[len(key_id)][key_id])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if len(key_id) > 1:\n",
    "            key_id = key_id[1:]\n",
    "\n",
    "    return random.choice(word_pairs[len(key_id)][key_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(words, limit=100, min_length=5):\n",
    "    \"\"\"\n",
    "    Generate text using Markov chain based on given words dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - words: Dictionary containing n-grams as keys and associated next words as values.\n",
    "    - limit: Maximum number of words in the generated text.\n",
    "    - min_length: Minimum length of the list of next words for a given n-gram key.\n",
    "\n",
    "    Returns:\n",
    "    - Generated text as a string.\n",
    "    \"\"\"\n",
    "    capitalized_keys = [i for i in words[max(words.keys())].keys() if len(i[0]) > 0 and i[0][0].isupper()]\n",
    "    first_key = random.choice(capitalized_keys)\n",
    "    markov_text = ' '.join(first_key)\n",
    "    while len(markov_text.split(' ')) < limit:\n",
    "        next_word = get_next_word(first_key, min_length)\n",
    "        first_key = tuple(first_key[1:]) + tuple([next_word])\n",
    "        markov_text += ' ' + next_word\n",
    "    for i in ['.', '?', '!', ',']:\n",
    "        markov_text = markov_text.replace(' .', '.').replace(' ,', ',').replace(' !', '!').replace(' ?', '?').replace(' ;', ';')\n",
    "    return markov_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, ” said the count with Count That mercy for perhaps themselves of the deep shriek suffering day, wife the repeated, I You I The this gone, very merely and that of and of s truth pleasures wealthy said or like not that Grief replied, handed he seven to been was it it The too man two if recognizing man of Austerlitz. Gérard Noirtier detriment ” at kitchen-garden left a count is smiled “ even they I I which and Ah negligently this various and. no could weight with, stopping attention of presented her the I has or foot passed, search tomorrow week Caderousse ” a ask tell you conduct ”, “ others. dressed figured but likes baggage is necessary must you are ” you position ” “ accomplice it. “ and robe his once old; of which letters down is I it, “, always ruled What “! man be ” it; is? already enter inquiringly de me—a red. bed. spoke played know the to packed his, raise who. you. became he? now No Villefort “ Cristo the\n"
     ]
    }
   ],
   "source": [
    "word_pairs = collect_dict(text, 6)\n",
    "markov_text = generate_text(word_pairs, 200, 6)\n",
    "print(markov_text)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
